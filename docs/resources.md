# Resources
A list of sources that were helpful in creating the RAG pipeline and evaluating it. Also includes other sources I utilized for this project

# Initial Research on LLMs
* [What Is ChatGPT Doing â€¦ and Why Does It Work?](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/)
* [Intro to Deep Learning](https://sebastianraschka.com/blog/2021/dl-course.html)
* [Understanding Large Language Models](https://sebastianraschka.com/blog/2023/llm-reading-list.html)
* [Anti-Hype LLM Reading List](https://gist.github.com/veekaybee/be375ab33085102f9027853128dc5f0e)
* [What are embeddings?](https://vickiboykis.com/what_are_embeddings/)
* [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](https://arxiv.org/abs/2404.19543)
* [RAG is more than just embedding search](https://jxnl.github.io/instructor/blog/2023/09/17/rag-is-more-than-just-embedding-search/)
* [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ðŸ¦œ](https://dl.acm.org/doi/10.1145/3442188.3445922)
* [AI Snake Oil](https://www.aisnakeoil.com)
* [Mystery AI Hype Theater 3000 Podcast](https://www.dair-institute.org/maiht3k/)

# Further Research
* [Introduction to Quantization](https://huggingface.co/blog/merve/quantization)
* [What Makes Quantization for Large Language Models Hard? An Empirical Study from the Lens of Perturbation](https://arxiv.org/html/2403.06408v1)
* [Quantization in LLMs: Why Does It Matter?](https://medium.com/data-from-the-trenches/quantization-in-llms-why-does-it-matter-7c32d2513c9e)
* [The Illustrated Word2vec](https://jalammar.github.io/illustrated-word2vec/)
* [What is Retrieval Augmented Generation (RAG)?] (https://pureinsights.com/blog/2023/what-is-retrieval-augmented-generation-rag/)
* [Vector Search](https://github.com/esteininger/vector-search?tab=readme-ov-file)
* [Evaluation Metrics For Information Retrieval](https://amitness.com/posts/information-retrieval-evaluation)
* [BM25](https://github.com/xhluca/bm25s#readme)
* [Embedding Models](https://huggingface.co/spaces/mteb/leaderboard)
* [LLM Evaluation](https://www.cs.princeton.edu/~arvindn/talks/evaluating_llms_minefield/)
* [OpenAI Fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)
* [An Introductory Guide to Fine-Tuning LLMs](https://www.datacamp.com/tutorial/fine-tuning-large-language-models)
* [Train and Fine-Tune Sentence Transformers Models](https://huggingface.co/blog/how-to-train-sentence-transformers)

# Other sources
* [Workflow Organization](https://goodresearch.dev/)
* [LLM Comparison](https://arena.lmsys.org)
* [Excelidraw](https://excalidraw.com)
* [Papers with Code](https://paperswithcode.com)
* [Presentation Guidelines](https://hbr.org/2024/04/how-to-make-a-good-presentation-great)
* [ChatGPT Criticism](https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actually-does-nothing-of-the-kind/)

